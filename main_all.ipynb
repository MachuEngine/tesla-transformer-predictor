{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\stt_env\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Sample: [[1.46206675e+01 7.14660000e+07 1.48579998e+01 1.48833332e+01\n",
      "  1.42173328e+01]\n",
      " [1.40059996e+01 8.05275000e+07 1.43033333e+01 1.44333334e+01\n",
      "  1.38106670e+01]\n",
      " [1.40853329e+01 9.39285000e+07 1.40039997e+01 1.42799997e+01\n",
      "  1.36140003e+01]\n",
      " [1.40633326e+01 4.45260000e+07 1.42233334e+01 1.43186674e+01\n",
      "  1.39853334e+01]\n",
      " [1.40413332e+01 5.16375000e+07 1.41873331e+01 1.42533331e+01\n",
      "  1.40006666e+01]]\n",
      "Data Shape: (2538, 5)\n",
      "Train Data Shape: (2030, 5)\n",
      "Test Data Shape: (508, 5)\n",
      "Epoch 1/50, Loss: 0.148765, LR: 0.001000\n",
      "Epoch 2/50, Loss: 0.016860, LR: 0.001000\n",
      "Epoch 3/50, Loss: 0.011966, LR: 0.001000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 1. 데이터 로딩 및 전처리\n",
    "def load_and_preprocess_data(ticker, start_date, end_date, features=['Close', 'Volume', 'Open', 'High', 'Low'], split_ratio=0.8):\n",
    "    df = yf.download(ticker, start=start_date, end=end_date)\n",
    "    data = df[features].values  # [num_samples, num_features]\n",
    "    print(f\"Data Sample: {data[:5]}\")\n",
    "    print(f\"Data Shape: {data.shape}\")\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "    split_idx = int(len(data_scaled) * split_ratio)\n",
    "    train_data = data_scaled[:split_idx]\n",
    "    test_data = data_scaled[split_idx:]\n",
    "    print(f\"Train Data Shape: {train_data.shape}\")\n",
    "    print(f\"Test Data Shape: {test_data.shape}\")\n",
    "\n",
    "    return train_data, test_data, scaler\n",
    "\n",
    "\"\"\"\n",
    "이전과 달라진 부분 없다. data를 다운로드하고 MinMaxScaler를 통해 정규화한 후 train_data와 test_data로 분리하여 반환하는 함수이다. \n",
    "\"\"\"\n",
    "\n",
    "# 2. Dataset 및 DataLoader 구성 (멀티스텝 예측 버전)\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, seq_length, pred_length):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "        self.seq_length = seq_length\n",
    "        self.pred_length = pred_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length - self.pred_length + 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src = self.data[idx: idx+self.seq_length]\n",
    "        tgt = self.data[idx+self.seq_length: idx+self.seq_length+self.pred_length]\n",
    "        return src, tgt\n",
    "    \n",
    "\"\"\"\n",
    "create_dataloader() 함수에서 호출하는 TimeSeriesDataset(Dataset) 클래스이다.\n",
    "\n",
    "*. 여기서는 단일 스탭 예측에서 멀티 스텝 예측으로 target 부분이 변경되었다. 변경된 세부 내용은 아래와 같다.\n",
    "\n",
    "1) __init__ : data를 torch.tensor로 변환하고 seq_length와 pred_length를 저장한다. 여기서 data에는 train_data가 전달된다.\n",
    "2) __len__ : 데이터의 길이(유효 샘플 개수)를 반환한다. 여기서는 seq_length와 pred_length를 고려하여 반환한다.\n",
    "    전체 데이터 길이에서 seq_length와 pred_length를 빼는 이유\n",
    "        - seq_length는 입력 시퀀스 길이고, pred_length는 예측할 미래 시점의 길이이다.\n",
    "        - 이 방법은 멀티 스텝 예측 방법이며, \n",
    "        - 예를 들어, seq_length=50, pred_length=10이라면, 0~49번째 데이터로 50~59번째 데이터를 예측하고, \n",
    "          10~49번째 데이터 + 50~59번째 데이터로 60~69번째 데이터를 예측하는 식으로 진행한다.\n",
    "\n",
    "3) __getitem__ : 인덱스를 입력받아 해당 인덱스에 대한 데이터를 반환한다. \n",
    "    src는 seq_length만큼의 데이터를 반환하고, \n",
    "    tgt는 seq_length만큼의 데이터 이후부터 pred_length만큼의 데이터를 반환한다.\n",
    "\"\"\"\n",
    "\n",
    "def create_dataloader(data, seq_length, pred_length, batch_size, shuffle=True):\n",
    "    dataset = TimeSeriesDataset(data, seq_length, pred_length)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return loader\n",
    "\n",
    "\"\"\"\n",
    "create_dataloader() 함수는 TimeSeriesDataset을 활용하여 data set을 만들고,\n",
    "DataLoader를 통해 batch_size 만큼 데이터를 불러올 수 있는 loader 객체를 생성한다.\n",
    "\n",
    "여기서 dataset은 train_data를 TimeSeriesDataset 클래스로 변환한 객체이다.\n",
    "그리고 src, tgt를 반환하는데, 이는 TimeSeriesDataset 클래스의 __getitem__ 메서드에서 정의한 대로 반환된다.\n",
    "따라서 loader를 통해 데이터를 불러올 때마다 src와 tgt가 batch_size만큼 튜플 형태로 반환된다.\n",
    "\n",
    "loader의 shape는 다음과 같다.\n",
    "    - src : [batch_size, seq_length, input_dim]\n",
    "    - tgt : [batch_size, pred_length, input_dim]\n",
    "\"\"\"\n",
    "\n",
    "# 3. Transformer 기반 시계열 예측 모델 정의 (멀티스텝 예측 지원)\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, num_layers, dropout=0.1):\n",
    "        super(TimeSeriesTransformer, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, input_dim)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # src: [S, N, input_dim], tgt: [T, N, input_dim]\n",
    "        src_emb = self.embedding(src)   # [S, N, d_model]\n",
    "        tgt_emb = self.embedding(tgt)     # [T, N, d_model]\n",
    "        output = self.transformer(src_emb, tgt_emb)  # [T, N, d_model]\n",
    "        return self.fc_out(output)      # [T, N, input_dim]\n",
    "\n",
    "\"\"\"\n",
    "TimeSeriesTransformer 클래스는 nn.Module을 상속받아 정의된 클래스이다. 모델 구조는 이전과 달라진 부분은 없다. \n",
    "\"\"\"\n",
    "\n",
    "# 4. 모델 학습 함수 (멀티스텝 예측 및 Teacher Forcing 적용)\n",
    "def train_model(model, train_loader, device, epochs, learning_rate=0.001, teacher_forcing_ratio=0.5):\n",
    "    model.train()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for src, tgt in train_loader:\n",
    "            src = src.to(device)   # [batch, seq_length, input_dim]\n",
    "            tgt = tgt.to(device)   # [batch, pred_length, input_dim]\n",
    "            \n",
    "            src = src.transpose(0, 1)  # [seq_length, batch, input_dim] <- Transformer 입력 형태로 reshape\n",
    "            batch_size = src.size(1) # decode 함수에서 사용하기 위해 batch_size 저장\n",
    "            input_dim = src.size(2) # decode 함수에서 사용하기 위해 input_dim 저장\n",
    "            pred_length = tgt.size(1) # decode 함수에서 사용하기 위해 pred_length 저장\n",
    "            \n",
    "            # teacher forcing 적용 여부 결정\n",
    "            use_teacher_forcing = True if np.random.rand() < teacher_forcing_ratio else False\n",
    "            \n",
    "            if use_teacher_forcing:\n",
    "                # 디코더 입력: 시작 토큰(0벡터) + 타겟 시퀀스의 앞쪽 토큰들\n",
    "                start_token = torch.zeros(1, batch_size, input_dim).to(device)\n",
    "                tgt_transposed = tgt.transpose(0, 1)  # [pred_length, batch, input_dim]\n",
    "                decoder_input = torch.cat([start_token, tgt_transposed[:-1]], dim=0)  # [pred_length + 1, batch, input_dim]\n",
    "\n",
    "                # Slicing tgt_transposed[:-1] is equivalent to removing the last element of tgt_transposed\n",
    "                # 슬라이싱 문법 [start:end]을 사용하면, 기본적으로 첫 번째 차원(시간 차원)이 조작됩니다.\n",
    "                # tgt_transposed : [pred_length, batch, input_dim]\n",
    "                # tgt_transposed[:-1] : [pred_length-1, batch, input_dim]\n",
    "\n",
    "                # decoder_input : start_token(0벡터) + tgt_transposed[:-1] = [pred_length, batch, input_dim]\n",
    "                # decoder_input : [1, batch, input_dim] + [pred_length-1, batch, input_dim] = [pred_length, batch, input_dim]\n",
    "            else:\n",
    "                # teacher forcing 없이 0벡터만 사용\n",
    "                decoder_input = torch.zeros(pred_length, batch_size, input_dim).to(device)\n",
    "\n",
    "                # decoder_input : [pred_length, batch, input_dim]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, decoder_input)  # [pred_length, batch, input_dim]\n",
    "            loss = criterion(output, tgt.transpose(0, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss / len(train_loader):.6f}, LR: {current_lr:.6f}\")\n",
    "\n",
    "\"\"\"\n",
    "train_model() 에서는 teacher forcing을 적용하여 모델을 학습하는 부분이 추가되었다.\n",
    "\n",
    "teacher forcing은 디코더의 입력을 타겟 시퀀스의 앞쪽 토큰들로 주는 방법이다.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 5. 미래 예측 (멀티스텝 Rollout 방식)\n",
    "def predict_future(model, test_data, seq_length, pred_length, total_predictions, device):\n",
    "    model.eval()\n",
    "    test_input = torch.tensor(test_data[:seq_length], dtype=torch.float32).to(device)  # [seq_length, input_dim]\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        steps = total_predictions // pred_length\n",
    "        remainder = total_predictions % pred_length\n",
    "        for _ in range(steps):\n",
    "            src = test_input.unsqueeze(1)  # [seq_length, 1, input_dim]\n",
    "            decoder_input = torch.zeros(pred_length, 1, test_input.size(-1)).to(device) # sos token - 0벡터\n",
    "            out = model(src, decoder_input)    # [pred_length, 1, input_dim]\n",
    "            out = out.squeeze(1)               # [pred_length, input_dim]\n",
    "            predictions.append(out.cpu().numpy())\n",
    "            # 예측된 구간을 시퀀스에 추가하여 롤아웃 업데이트한다.\n",
    "            test_input = torch.cat([test_input[pred_length:], out], dim=0)\n",
    "        if remainder > 0:\n",
    "            src = test_input.unsqueeze(1)\n",
    "            decoder_input = torch.zeros(remainder, 1, test_input.size(-1)).to(device) # sos token - 0벡터\n",
    "            out = model(src, decoder_input)\n",
    "            out = out.squeeze(1)\n",
    "            predictions.append(out.cpu().numpy())\n",
    "        predictions = np.concatenate(predictions, axis=0)\n",
    "    return predictions\n",
    "\n",
    "\"\"\"\n",
    "1) test_input : test_data의 앞쪽 seq_length만큼을 입력으로 사용한다.\n",
    "2) steps : total_predictions을 pred_length로 나눈 몫을 저장한다.\n",
    "3) remainder : total_predictions을 pred_length로 나눈 나머지를 저장한다.\n",
    "4) for문에서 steps만큼 반복하면서 예측을 수행한다.\n",
    "    - src : test_input을 unsqueeze하여 차원을 추가한다. [seq_length, 1, input_dim]\n",
    "    - decoder_input : [pred_length, 1, input_dim] 형태의 0벡터를 생성한다.\n",
    "    - out : 모델에 src와 decoder_input을 입력하여 예측을 수행한다. [pred_length, 1, input_dim]\n",
    "    - out : squeeze를 통해 차원을 줄인다. [pred_length, input_dim]\n",
    "    - predictions : 예측 결과를 리스트에 추가한다.\n",
    "    - test_input : 예측된 구간을 시퀀스에 추가하여 롤아웃 업데이트한다.\n",
    "5) remainder가 0보다 크다면, 나머지에 대한 예측을 수행한다.\n",
    "6) 최종 예측 결과를 반환한다.\n",
    "\"\"\"\n",
    "\n",
    "# 6. 결과 시각화 함수 (Close price만)\n",
    "def plot_predictions(actual, predictions, seq_length):\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(actual[:, 0], label=\"Actual Close Price\")\n",
    "    plt.plot(range(seq_length, seq_length + len(predictions)), predictions[:, 0], \n",
    "             label=\"Predicted Close Price\", linestyle=\"dashed\")\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.title(\"Tesla Stock Price Prediction using Transformer\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# 7. 메인 실행 함수\n",
    "def main():\n",
    "    # 설정값\n",
    "    ticker = \"TSLA\"\n",
    "    start_date = \"2015-01-01\"\n",
    "    end_date = \"2025-02-05\"\n",
    "    seq_length = 60       # 입력 시퀀스 길이\n",
    "    pred_length = 20      # 한 번에 예측할 미래 시점 개수 (멀티스텝 예측)\n",
    "    batch_size = 32\n",
    "    epochs = 50\n",
    "    total_predictions = 200   # 예측할 총 미래 시점 개수\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # 데이터 로딩 및 전처리\n",
    "    train_data, test_data, scaler = load_and_preprocess_data(\n",
    "        ticker, start_date, end_date,\n",
    "        features=['Close', 'Volume', 'Open', 'High', 'Low'],\n",
    "        split_ratio=0.8\n",
    "    )\n",
    "    \n",
    "    # DataLoader 생성 (멀티스텝 예측)\n",
    "    train_loader = create_dataloader(train_data, seq_length, pred_length, batch_size)\n",
    "    \n",
    "    # 모델 생성\n",
    "    model = TimeSeriesTransformer(input_dim=5, d_model=128, nhead=4, num_layers=3).to(device)\n",
    "    \n",
    "    # 모델 학습 (멀티스텝 및 Teacher Forcing 적용)\n",
    "    train_model(model, train_loader, device, epochs, learning_rate=0.001, teacher_forcing_ratio=0.5)\n",
    "    \n",
    "    # 미래 예측 (멀티스텝 Rollout)\n",
    "    predictions = predict_future(model, test_data, seq_length, pred_length, total_predictions, device)\n",
    "    predictions_inverse = scaler.inverse_transform(predictions)\n",
    "    actual_test = scaler.inverse_transform(test_data)\n",
    "    \n",
    "    # 결과 시각화 (Close price만)\n",
    "    plot_predictions(actual_test, predictions_inverse, seq_length)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
